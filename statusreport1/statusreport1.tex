%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2010 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2010,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2010, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2010} with
% \usepackage[nohyperref]{icml2010} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
% \usepackage{icml2010} 
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Appearing in''
\usepackage[accepted]{icml2010}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{CS 6784: Project proposal}



\begin{document} 

\twocolumn[
\icmltitle{CS 6784 (Spring 2014): Project proposal}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2010
% package.
\icmlauthor{Charles Hermann}{cih5}
%\icmladdress{Your Fantastic Institute,
%            314159 Pi St., Palo Alto, CA 94306 USA}
\icmlauthor{Jonathan Shi}{js2845}
%\icmladdress{Their Fantastic Institute,
%            27182 Exp St., Toronto, ON M6H 2T1 CANADA}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{project proposal, MIR, transcription, machine learning}

\vskip 0.3in
]

%\begin{abstract} 
%ICML 2010 full paper submissions are due February 1, 2010. Reviewing will
%be blind to the identities of the authors, and therefore identifying
%information should not appear in any way in papers submitted for
%review. Submissions must be in PDF, 8 page length limit.
%\end{abstract} 


\section{Team}
Charles Hermann (cih5) and Jonathan Shi (js2845)


\section{Motivation}
One of the major areas of research in MIR is music transcription, the
process of creating a musical score from an audio recording. Though this
can be done manually by experienced musicians, this process is difficult
and time consuming. As a result, the ability to automatically transcribe
music has numerous positive ramifications in the musical community and the
MIR community. In addition, transcription presents an interesting and
difficult challenge even for seemingly easily versions of the task such as
transcribing piano solos. Several factors make this a difficult area of
study including but not limited to: the high-dimensionality of the
datasets, the large margin for error (both false positives and false
negatives), and the lack of a field-accepted feature vector/technique for
extracting the data. 

Currently, the primary techniques used in transcription are HMMs applied to
temporal windows and frequency bins \cite{raphael2002automatic}, neural
networks \cite{bock2012polyphonic}, and simple
discriminative techniques to identify if a singular note is being played in
a temporal window \cite{poliner2006discriminative}. Also feature sets have
been obtained via deep learning \cite{nam2011classification} and sparse
nonnegative matrix factorization \cite{costantini2013nmf}, which have performed
better in classification-based transcription than manually specified features.
Finally, techniques have been tuned to detect note onsets and note offsets for
the piano \cite{5946322}.


\section{Statement of Task}
\begin{enumerate}
\item
Can we improve upon the state-of-the-art in transcription? Specifically,
how well will techniques such as max-margin markov models (involving a
multi-class SVM for note identification and a HMM on top of identified notes)
work within this area?
\item
Can we approach transcription as a streaming problem?
\item
Can we create an interesting or novel transcription technique which
involves human suggestions? For example, can we use a resource like
\url{http://www.ultimate-guitar.com} to aid in transcription of more
complicated songs? For another example, can we increase the accuracy of
transcription using human input?
\end{enumerate}


\section{General Approach}
\begin{enumerate}
\item
One approach we intend to try to use to improve upon existing automated
transcription techniques is by using a
maximum-margin Markov model \cite{Taskar03max-marginmarkov} which improves
upon an HMM by using SVM techniques to find optimal separating hyperplanes
that respect correlations between components of output labels.
In addition, we expect the maximum-margin Markov framework introduced in
\cite{anguelov2005discriminative} to be of use.
\item
Two factors in current work prevent the techniques from applying to
transcription as a streaming problem: first, assumptions made in the
algorithms which try to use future notes to better estimate current notes,
and second, the large amount of computation needed for
current techniques and problems with processing power. It seems like if we
could remove the assumption that future sound data is available, and make
simplifications to the model with only a marginal loss in accuracy, this
could be an interesting and useful advancement/technique.
\item
Numerous resources exist online where people have partially or noisily
transcribed large amounts of data. For example,
\url{http://www.ultimate-guitar.com}
contains a large database of user generated guitar tablatures (``tabs'') for
popular songs. Unfortunately, these tabs often contain mistakes, scale
transformations, or simplifications of the actual song in
question. However, it is possible these tabs could be used as a starting
point to inform a more accurate transcription.

We also wonder if human input could usefully solve
some of the more difficult problems in music transcription.
For example, one of the main steps in transcription is determining
note onset (when a note is first played). While this is easy for humans,
this has proven difficult for machines due to secondary harmonics mimicking
the onset of false notes. Perhaps a Bayesian approach which asked humans to
help when confronted with low certainty could be useful. This way, with the
aid of computers, even non-experts could quickly generate transcriptions for
more complicated music. We would be lowering the experience level needed to
generate the transcriptions, and make this a more accessible task for amateur
musicians who might want to, for instance, create a score to play their
instrument off of.
\end{enumerate}


\section{Resources}
There are several datasets that should be applicable to our task on
\url{http://deeplearning.net/datasets/}. In particular, MIDI sequences and
associated synthesized audio files can be found on the web pages
\url{http://www.piano-midi.de/} and \url{http://musedata.stanford.edu/}.
Researcher-generated datasets used in \cite{poliner2006discriminative} and
\cite{emiya2010multipitch} are available and used in other papers in the area.

User-created tabs of songs can be found on \url{http://www.ultimate-guitar.com}
and 30-second samples of these can be found on \url{http://us.7digital.com/}.

We also expect all of our cited works to contribute valuable insight and
techniques, whether in classification methods or in feature extraction.


\section{Schedule}
Our milestones for this project are:
\begin{description}
\item[16 Feb.] Project proposal submitted.
\item[ 1 Mar.] Literature reviewed, datasets acquired.
\item[15 Mar.] Features acquired, and max-margin Markov model implemented.
\item[16 Mar.] First status report submitted.
\item[ 1 Apr.] Several relaxations attempted for streaming transcription
  possible.
\item[15 Apr.] Model modified to allow for human input and additional
  ``hints'', and to detect when human input may be necessary.
\item[20 Apr.] Second status report submitted.
\item[25 Apr.] Basic UI implemented for easy use of human-assisted machine
  transcription.
\item[30 Apr.] Class presentation.
\item[12 May.] Final project report submitted.
\end{description}


\section{Changes to the proposal}

\subsubsection{Not yet changed}
Getting rid of the online learning part.

Adding more precise evaluation methods.

Discussion of possible ways to tune the features.

Possibility of using derivatives as features.

\subsubsection{Actually changed}


\section{Literature review}
We finished our literature review of the current methods used in piano music
transcription. We found that the current state of the art methods used are
<blah>. In particular, there are a variety of signal processing techniques used
to acquire feature sets are including:
\begin{description}
\item[Short-term Fourier transform (STFT).]
  This involves taking a set of discrete time-localized windows of the original
  signal, and applying the Fourier transform to each window, so as to get an
  analysis of the frequencies roughly present at particular times
  \cite{bock2012polyphonic} \cite{poliner2006discriminative}.
\item[Wavelet transform.]
  This is similar to the STFT, but differs in that while the STFT decomposes
  time-local windows of the signal into frequency-local sinusoidal basis
  functions, the wavelet transform decomposes the whole signal into roughly
  time-and-frequency-local ``wavelet'' basis functions. The main effect of this
  is that it offers better time resolution at high frequencies (since
  high-frequency wavelets will tend to be more time-localized), and better
  frequency resolution at low frequencies. Because the shape of the wavelet
  functions can be customized, it is possible that wavelets could also be used
  to decompose a signal into more application-relevant bases.
\item[Constant Q transform.] 
  The Constant Q transform is closely related to a particular type of wavelet
  transform: the Morlet wavelet transform. The Morlet wavelet is a wavelet
  consisting of a complex-exponential/sinusoidal wave attenuated by a
  Gaussian envelope. Similarly, the Constant Q transform takes a STFT, and
  instead of using discrete rectangular windows, uses Gaussian windows whose
  time-widths vary with the particular frequency bucket being analyzed.
  This transform is reportedly well suited for music-related applications,
  since the action of this transform mirrors that of human perception
  \cite{5946322} \cite{costantini2013nmf}.
\item[Fine-tuned spectral features.]
  Sometimes previous work has incorportated feature sets from spectrograph
  data that have been specially tuned for the transcription task.
  These include a pitch salience function \cite{5946322}, deeply learned
  feature sets \cite{nam2011classification}, features acquired via sparse
  nonnegative matrix factorization \cite{costantini2013nmf}, and features
  generated by a model of human inner ear hair cells 
  \cite{marolt2004connectionist}. 
\end{description}

Using any of the above methods, we obtain spectograms, which are plots of which
particular frequencies are present at any particular time within a signal.
Observable features within a spectogram include:
\begin{description}
\item[Overtones.]
  Each note, when played, consists of a fundamental frequency, which is the
  pitch of the note that we perceive, as well as a set of overtones at
  integer multiples of the original frequency.
\item[Partials.]
  A partial of a note is one of the theoretically pure sine wave constituents of
  that note. The fundamental frequency will be one of the partials, and each
  overtone will contribute another partial. Rather than being pure sine waves,
  which would show up in the spectograph as straight think horizontal lines, in
  reality each partial will be a bit fuzzy and show up as blurred horizontal
  bars in the spectograph.
\item[Missing fundamentals.] 
  In certain cases, a human will hear a note being played at a certain pitch
  even if the fundamental frequency is omitted from the signal. This is because
  the human audio processing system can infer the fundamental frequency from
  the present overtones. 
\item[Beat frequencies.]
  When two distinct frequencies are played over each other, they give rise to
  a beat frequency equal to the difference of the two original frequencies.
  Human ears hear these as subjective tones, and some previous attempts have
  incorrectly detected beat frequencies as new note onsets
  \cite{marolt2004connectionist}. 
\end{description}

Difficult points for previous classifiers have also included octave errors,
since notes that are an octave apart from each other are very similar in
their partials \cite{bock2012polyphonic} \cite{poliner2006discriminative}.
Previous methods also experienced
difficulties in distinguishing which notes were onset and which were just held
when some note onset event was detected \cite{marolt2004connectionist}.

Previous work on this task have focused mostly around:
\begin{enumerate}
\item Markov networks, SVMs, and CRFs trained to detect particular notes or
  note events
  \cite{ryynanen2005polyphonic} \cite{poliner2006discriminative}
  \cite{gang2009polyphonic}.
\item Neural network approaches using recurrent networks
  \cite{marolt2004connectionist} \cite{bock2012polyphonic}.
\item Estimation and subtraction, or expectation-maximization, of sets of
  overtones for detected fundamental frequencies
  \cite{5946322} \cite{}.
\end{enumerate}

Manual inspection of a spectrograph of a piano playing reveals that some
partials can fade more quickly than others, and some partials can twinkle out
of existence for brief moments before reappearing within the same note strike.

We were pointed to another paper \cite{gang2009polyphonic} dealing with the use
of max-margin methods to transcribe musical notes. Our approach differs
substantially from theirs because they apply a max-margin markov network to
classify sets of pre-segmented partials according to which instrument they
belong to. Meanwhile we skip the analysis of partials and attempt to classify
notes directly from the spectograms as features. 


\section{Refinement of ideas}
We've narrowed our focus and elaborated our ideas since the project proposal.
For instance, we've decided to focus on implementation of a max-margin markov
network for transcribing particular notes given spectographic data as features.
We understand the basic steps that need to be implemented (transforming the
signal, possibily with downsampling, and feeding the transformed signal
directly as features into a classifier). The structured output consists of
a set of labels for which notes are present in each time window, exploiting
correlations between notes both for different frequencies at the same time and
for the same or similar frequencies at different times.

We've established a concrete plan to evaluate our results.


\section{Data}
We've obtained access to the datasets used in \cite{poliner2006discriminative}
and \cite{marolt2004connectionist}. There are also a large number of piano
mp3s and midi files available at \url{http:/www.piano-midi.de/}, though these
require significant processing for use as experimental or training data.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{langley00}

\bibliography{statusreport1}
\bibliographystyle{icml2010}

\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This 2010 version was
% created by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  


